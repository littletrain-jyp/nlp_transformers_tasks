dataset:
  train_dataset: ./data/cnews/cnews.train.txt
  eval_dataset: ./data/cnews/cnews.val.txt
  test_dataset: ./data/cnews/cnews.test.txt
  label_dir: ./data/cnews/labels.txt

model:
  pretrained_name_or_path: hfl/chinese-bert-wwm-ext # ptrsxu/chinese-bert-wwm-ext # hfl/chinese-bert-wwm-ext # uer/albert-base-chinese-cluecorpussmall

run:
  device_id: '1' # 使用cpu则置为-1

train:
  train_epochs: 2
  per_device_batch_size: 16
  logging_steps: 5
  eval_steps: 400

  select_model_metric: 'macro_f1'

  use_class_weight: False # 是否按类别数量权重更新loss
  class_weight_max_scale_ratio: 10.0 # 最大扩充比例

  use_adversarial: True # 默认FGM

  with_amp: True # 混合精度训练

  max_length: 512 #最长字数限制

  learning_rate: 3.0e-5
  gradient_accumulation_steps: 2
  warmup_ratio: 0.01
  max_grad_norm: 1.0
  weight_decay: 0.01

  adam_epsilon: 1e-8






